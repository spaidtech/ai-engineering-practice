# ğŸ“˜ MODEL SELECTION, VALIDATION & BIASâ€“VARIANCE (MICRO NOTES)

This document is **lock-in knowledge** for Applied ML Engineers.  
If you understand and remember this â€” you are safe in **projects + interviews**.

---

## ğŸ§  PART A â€” MODEL SELECTION

### ğŸ”¹ What is Model Selection?
Choosing **which model family** to use for a problem.

You are **NOT**:
- tuning hyperparameters
- checking final accuracy

You are deciding:   
> **â€œWhich type of model can solve this problem?â€**

---

### ğŸ”¹ Why Model Selection is Needed
Different models behave differently:
- Simple vs complex  
- Linear vs non-linear  
- Low vs high variance  

âŒ Wrong model â†’ wasted time + poor results

---

### ğŸ”¹ Naive but Correct Rule
Start simple  
â†“  
Increase complexity **only if needed**

---

### ğŸ”¹ Practical Model Order (MEMORIZE)

| Problem Type | Start With | Upgrade If Needed |
|--------------|-----------|-------------------|
| Regression | Linear Regression | Tree â†’ RF â†’ Boosting |
| Classification | Logistic Regression | Tree â†’ RF â†’ Boosting |
| Non-linear data | Decision Tree | Random Forest |
| Very large data | Linear / Tree | Neural Network |

---

### ğŸ”¹ What to Check During Selection?
âœ” **Training performance only**

| Training Error | Meaning |
|---------------|--------|
| High | Model too simple |
| Reasonable | Good candidate |
| Very low | Might overfit (check later) |

---

### ğŸ”¹ Model Selection Summary
- Selection decides **WHAT model**
- Not **how good** yet

---

## ğŸ§ª PART B â€” MODEL VALIDATION

### ğŸ”¹ What is Validation?
Testing model performance on **unseen data**

ğŸ“Œ Training performance lies  
ğŸ“Œ Validation performance tells truth

---

### ğŸ”¹ Why Validation is Needed
Models can:
- memorize training data  
- fail on new data  

Validation detects this failure.

---

### ğŸ”¹ Correct Data Usage (VERY IMPORTANT)

| Split | Purpose |
|-----|--------|
| Train | Learn |
| Validation | Check |
| Test | Final exam (ONCE) |

âŒ Never tune using test data

---

### ğŸ”¹ Simplest Validation â€” Hold-out
Split:
- Train: 70%
- Validation: 15%
- Test: 15%

| Train | Val | Meaning |
|------|-----|--------|
| High | Low | Overfitting |
| Low | Low | Underfitting |
| High | High | Good model |

---

### ğŸ”¹ Better Validation â€” Cross-Validation
- Split data into K folds  
- Train K times  
- Average validation score  

âœ” Stable  
âœ” Reliable  
âœ” Industry standard

---

### ğŸ”¹ What Validation Tells You

| Observation | Conclusion |
|-----------|-----------|
| Train â‰« Val | Overfitting |
| Train â‰ˆ Val (low) | Model too simple |
| Train â‰ˆ Val (high) | Best model |

---

### ğŸ”¹ Validation Summary
Validation decides:
> **â€œCan I trust this model?â€**

---

## ğŸ§© BIG PICTURE (ONE LINE)
**Select â†’ Validate â†’ Tune â†’ Test â†’ Deploy**

---

## ğŸ§  ONE-PAGE MEMORY RULE
- Selection â†’ Which model?  
- Validation â†’ Does it generalize?  
- Test â†’ Final proof only  

---

## ğŸ” CROSS-VALIDATION (CODE + MEANING)

### 1ï¸âƒ£ Basic K-Fold (Most Common)

```python
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(max_iter=1000)

scores = cross_val_score(
    model, X, y,
    cv=5,
    scoring="accuracy"
)

print(scores.mean())
```

ğŸ§  Meaning:
- 5 splits  
- 5 trainings  
- Final score = average  

---

### 2ï¸âƒ£ Regression CV

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score

scores = cross_val_score(
    LinearRegression(),
    X, y,
    cv=5,
    scoring="neg_root_mean_squared_error"
)

rmse = -scores.mean()
```

ğŸ“Œ sklearn returns **negative RMSE**

---

### 3ï¸âƒ£ Manual KFold
Use when:
- reproducibility matters  
- shuffle control needed  

```python
from sklearn.model_selection import KFold
kf = KFold(n_splits=5, shuffle=True, random_state=42)
```

---

### 4ï¸âƒ£ Imbalanced Data â€” StratifiedKFold
ğŸ“Œ Keeps class ratio same in each fold

```python
from sklearn.model_selection import StratifiedKFold
```

---

### 5ï¸âƒ£ Time-Series CV
ğŸ“Œ Future data never predicts past

```python
from sklearn.model_selection import TimeSeriesSplit
```

---

### 6ï¸âƒ£ CV Selection Table (MEMORIZE)

| Situation | Use |
|----------|------|
| Normal data | KFold |
| Imbalanced | StratifiedKFold |
| Time-series | TimeSeriesSplit |
| Grouped users | GroupKFold |

---

### ğŸ”’ Sacred Rule
- CV â†’ selection & tuning  
- Test set â†’ final evaluation only  

---

# âš™ï¸ HYPERPARAMETER TUNING

### ğŸ”¹ What Are Hyperparameters?
Settings chosen before training that control:
- model complexity  
- learning behavior  

Examples:
- tree depth  
- learning rate  
- regularization strength  
- k in KNN  

---

### ğŸ”¹ Why Tuning Exists
Same model + same data â†’ different performance depending on hyperparameters

Goal:
- reduce bias  
- reduce variance  
- improve generalization  

ğŸ“Œ Tuning does NOT fix bad data

---

### ğŸ”¹ Biasâ€“Variance Connection

| Observation | Diagnosis | Action |
|-----------|---------|--------|
| High train & val error | High Bias | Increase complexity |
| Low train, high val | High Variance | Increase regularization |
| Train â‰ˆ Val & low | Good model | Stop |

---

### ğŸ”¹ Important Hyperparameters (Only What Matters)

#### Decision Tree
- max_depth  
- min_samples_leaf  

Overfit â†’ â†“ depth, â†‘ leaf  
Underfit â†’ â†‘ depth

#### Random Forest
- n_estimators  
- max_depth  
- min_samples_leaf  

More trees â†’ â†“ variance

#### Gradient Boosting
- learning_rate  
- n_estimators  
- max_depth  

ğŸ“Œ Low LR + more trees = best

#### Linear / Logistic
- alpha / lambda / C  

â†‘ regularization â†’ â†“ variance, â†‘ bias

#### KNN
- small k â†’ overfit  
- large k â†’ underfit  

---

### ğŸ”¹ Industry Workflow
1. Train baseline  
2. Compare train vs val  
3. Diagnose bias/variance  
4. Tune 1â€“2 params  
5. Re-evaluate  
6. Stop when improvement plateaus  

---

### ğŸ”¹ Search Methods

| Method | Notes |
|-------|------|
| Manual | Best for learning |
| Grid Search | Inefficient |
| Random Search | Default choice |
| Bayesian / AutoML | Use later |

ğŸ“Œ Random > Grid in practice

---

### ğŸ”¹ Common Mistakes
âŒ Tuning before preprocessing  
âŒ Tuning too many parameters  
âŒ Using test set  
âŒ Blind AutoML  

---

### ğŸ”¹ Interview One-Liners
- Hyperparameter tuning balances biasâ€“variance using validation data  
- I tune after diagnosing train vs validation error  
- Random search is more efficient than grid search  

---

# âš–ï¸ BIASâ€“VARIANCE TRADE-OFF

### ğŸ”¹ What Bias Means
- Model too simple  
- Wrong assumptions  
- Underfitting  

---

### ğŸ”¹ What Variance Means
- Model too complex  
- Learns noise  
- Overfitting  

---

### ğŸ”¹ Metric Diagnosis (MOST IMPORTANT)

| Train | Test | Diagnosis |
|------|------|-----------|
| High | High | Bias |
| Low | High | Variance |
| Low | Low | Good |

---

### ğŸ”¹ Learning Curve Interpretation

#### High Bias
- train & val high  
- curves close  
- more data âŒ  
- better model âœ…  

#### High Variance
- train low, val high  
- big gap  
- more data âœ…  
- regularization âœ…  

---

### ğŸ”¹ Fixing Strategy
**Fix Bias**
- add features  
- increase model complexity  
- reduce regularization  

**Fix Variance**
- simplify model  
- add regularization  
- collect more data  

---

### ğŸ”¹ Golden Engineer Workflow
Train  
â†’ Compare train vs test  
â†’ Diagnose bias/variance  
â†’ Plot learning curve  
â†’ Fix  
â†’ Re-evaluate  

---

## ğŸ”‘ FINAL ONE-LINE MODEL
Bias = model too simple  
Variance = model too sensitive  
**Goal = lowest test error, not lowest train error**
