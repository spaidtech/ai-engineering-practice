# ğŸ“˜ UNSUPERVISED LEARNING â€” COMPLETE NOTES (Theory + Code + Industry)

## â­ Table of Contents
1ï¸âƒ£ **Core Idea**  
2ï¸âƒ£ **Industry Use Cases**  
3ï¸âƒ£ **Supervised vs Unsupervised**  
4ï¸âƒ£ **Evaluation Philosophy**  
5ï¸âƒ£ **Preprocessing Theory + Pipeline**  
6ï¸âƒ£ **Feature Engineering**  
7ï¸âƒ£ **PCA**  
8ï¸âƒ£ **Visualization**  
9ï¸âƒ£ **Algorithm Selection**  
ğŸ”Ÿ **K-Means**  
1ï¸âƒ£1ï¸âƒ£ **DBSCAN**  
1ï¸âƒ£2ï¸âƒ£ **DBSCAN Fail Cases**  
1ï¸âƒ£3ï¸âƒ£ **Metrics**  
1ï¸âƒ£4ï¸âƒ£ **Real Business Usage**  
1ï¸âƒ£5ï¸âƒ£ **Demographics Rule**  
1ï¸âƒ£6ï¸âƒ£ **Deployment**  
ğŸ”’ **Final Lock**  

---

## 1ï¸âƒ£ What is Unsupervised Learning

Unsupervised learning discovers structure and patterns in data **without labels**.

- No target variable  
- No correct answer  
- No loss correction  

ğŸ¯ Goal â†’ **Discover useful structure, not truth**

---

## 2ï¸âƒ£ Industry Applications

- Customer segmentation  
- Fraud detection  
- User grouping  
- Recommendation systems  
- Feature understanding  
- Preprocessing for supervised learning  

---

## 3ï¸âƒ£ Supervised vs Unsupervised

| Aspect | Supervised | Unsupervised |
|--------|----------|-------------|
| Labels | Yes | No |
| Objective | Accuracy | Useful structure |
| Evaluation | Metrics | Visualization |
| Learning | Loss | Geometry |
| Output | Prediction | Segments |

ğŸ“Œ Supervised = correctness  
ğŸ“Œ Unsupervised = usefulness  

---

## 4ï¸âƒ£ Evaluation Philosophy

Metrics lie in unsupervised learning.

âœ” Visualization  
âœ” Business reasoning  
âœ” Metrics last  

ğŸ“Œ If visualization doesnâ€™t make sense â†’ Reject the model  

---

# UNSUPERVISED PREPROCESSING â€” COMPLETE THEORY

Preprocessing decides clustering quality.

---

## Why It Matters

Most algorithms are distance-based. Distance only works if:

- Scale is correct  
- Noise handled  
- Features comparable  

ğŸ“Œ No label feedback â†’ mistakes stay forever  

---

## Pipeline

```
Raw Data
â†“
Drop IDs
â†“
Numeric Only
â†“
Handle Missing
â†“
Fix Skew
â†“
Handle Outliers
â†“
Scale (MANDATORY)
â†“
PCA (optional)
```

---

## Steps

### Drop IDs
```
df = df.drop(columns=["CustomerID"], errors="ignore")
```

### Numeric First
```
df = df.select_dtypes(include="number")
```

### Missing Handling
```
df = df.fillna(df.median())
```

### Fix Skew
```
import numpy as np
df = np.log1p(df)
```

### Outliers Carefully
```
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
df = df[~((df < (Q1 - 1.5*IQR)) | (df > (Q3 + 1.5*IQR))).any(axis=1)]
```

### Scaling (MANDATORY)
```
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df)
```

---

## PCA

```
from sklearn.decomposition import PCA
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_scaled)
```

ğŸ“Œ PCA reveals structure. Doesnâ€™t create it.

---

## ğŸ”¥ Why Scaling is Done *Before* PCA (VERY IMPORTANT)

PCA works by analyzing **variance** in features.

- Features with larger numeric scale automatically have higher variance  
- PCA assumes all features are equally important  
- If you do NOT scale â†’ PCA becomes biased toward highâ€‘magnitude features

Example:
- income â†’ 0 â€“ 1,000,000  
- age â†’ 18 â€“ 70  

Without scaling:
ğŸ‘‰ PCA thinks income is â€œmore importantâ€ only because numbers are bigger  
ğŸ‘‰ Result â†’ Wrong principal components, wrong structure, wrong clusters

### âœ” Therefore:
- Always scale before PCA  
- StandardScaler is preferred

```
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(df)
```

ğŸ“Œ Rule To Remember:
> PCA â‰ˆ variance analysis  
> Variance depends on scale  
> Therefore â†’ Scale â†’ THEN PCA  

---

## Feature Engineering

Features = Model.

```
df["spend_to_income"] = df["spending"] / (df["income"] + 1)
```

---

## Visualization

```
plt.scatter(X_pca[:,0], X_pca[:,1])
```

Look for:

- Shape  
- Density  
- Noise  

---

## Algorithm Selection

### K-Means (distance)

Use when:
- Blob shaped clusters
- Similar density

### DBSCAN (density)

Use when:
- Irregular shapes
- Noise important
- Unknown K

ğŸ“Œ KMeans = Distance  
ğŸ“Œ DBSCAN = Density  

---

## K-Means

```
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3, random_state=42)
labels_km = kmeans.fit_predict(X_pca)
```

---

## DBSCAN

```
from sklearn.cluster import DBSCAN
dbscan = DBSCAN(eps=0.6, min_samples=5)
labels_db = dbscan.fit_predict(X_pca)
```

---

## Metrics (Support Only)

```
from sklearn.metrics import silhouette_score
silhouette_score(X_pca, labels_km)
```

---

## After Clustering (REAL VALUE)

```
df["cluster"] = labels_km
df.groupby("cluster").mean()
```

ğŸ“Œ Output = Business segments

---

## Deployment Pattern

- Train offline  
- Save scaler + PCA + model  
- Periodically tag users  

---

# FINAL LOCK

Unsupervised â‰  Accuracy  
Visualization > Metrics  
PCA helps reasoning  
KMeans = distance  
DBSCAN = density  
Real output = Segments  

---

### COMPLETE âœ”
